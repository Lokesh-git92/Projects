{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Wikipedia Pages: Map Reduce implemntation of \"grep\"\n",
    "Implementing a simplified version of the grep command-line utility to search for data in 54 megabytes worth of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of files inthe \"wiki\" directory\n",
      "999\n",
      "Bay_of_ConcepciC3B3n.html\n",
      "Bye_My_Boy.html\n",
      "Valentin_Yanin.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "#listdir returns name of entires in the dirctory as a list\n",
    "file_names = os.listdir(\"wiki\")\n",
    "#non of files in the directory\n",
    "print('Total no of files inthe \"wiki\" directory')\n",
    "print(len(file_names))    \n",
    "\n",
    "#printing first three results\n",
    "for i in range(3):\n",
    "    print(file_names[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the first file and storing each line in list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bay_of_ConcepciC3B3n.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "folder_name = \"wiki\"\n",
    "file_name = str(file_names[0])\n",
    "print(file_name)\n",
    "#os.path.join joins folder name \"/\" file name\n",
    "with open(os.path.join(folder_name, file_name)) as f:\n",
    "    \n",
    "    #each line in the directory is being read as list object \n",
    "    # and stored in a list\n",
    "    #f.readlines() returns iterable list\n",
    "    lines = [line for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding  custom map reduce function which is built using multiprocessing module to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "#functools has reduce()- reads results from the parallel processing\n",
    "import functools\n",
    "# using the Pool object to create and run a group of processes\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "#divides data into chunks \n",
    "#prepareing data for parallel processing\n",
    "def make_chunks(data, num_chunks):\n",
    "    \n",
    "    #math.ceil() rounds up \n",
    "    chunk_size = math.ceil(len(data) / num_chunks)\n",
    "    \n",
    "    #chunks_size is my step size\n",
    "    #each data in list is length chunksize\n",
    "    return [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n",
    "\n",
    "def map_reduce(data, num_processes, mapper, reducer):\n",
    "    chunks = make_chunks(data, num_processes)\n",
    "    #initializing pool object with no of processors \n",
    "    pool = Pool(num_processes)\n",
    "    #pool.map() executes mapper function on the each data in chunks-parallel\n",
    "    chunk_results = pool.map(mapper, chunks)\n",
    "    # The Pool.close() method prevents the addition of new processes to the pool\n",
    "    pool.close()\n",
    "    # Pool.join() method makes the main program wait for all processes to finish before continuing executing\n",
    "    pool.join()\n",
    "    #reduce() combines each result from parallel executions \n",
    "    #into a global result-acts two data at a time from left most\n",
    "    return functools.reduce(reducer, chunk_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting the total number of lines on all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499797"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_line_count(file_names):\n",
    "    total = 0\n",
    "    for fn in file_names:\n",
    "        #joining wiki/fn aand opening using context manager\n",
    "        with open(os.path.join(\"wiki\", fn)) as f:\n",
    "            total += len(f.readlines())\n",
    "    return total\n",
    "    \n",
    "def reduce_line_count(count1, count2):\n",
    "    return count1 + count2\n",
    "\n",
    "#8 processors are used here\n",
    "map_reduce(file_names, 8, map_line_count, reduce_line_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a target string , find which line it occurs in each file \n",
    "The goal is to locate all lines in all files from the wiki folder that contains a given string.\n",
    "\n",
    "The mapper function receives a chunk of filenames and calculates all occurrences of the target string on them. If a file contains no occurrences, it is chosen to not include an entry for that file in the result dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep(file_names):\n",
    "    results = {}\n",
    "    #iterable is name of the file\n",
    "    for fn in file_names:\n",
    "        \n",
    "        # opening one file and extracting each line\n",
    "        with open(fn) as f:\n",
    "            lines = [each for each in f.readlines()]\n",
    "        \n",
    "        #enumerate allows us to keep track with an index for an iterable\n",
    "        for line_index, line in enumerate(lines):\n",
    "            \n",
    "            #target is the key word that I am looking for in each line\n",
    "            #checking if the target string is already in the dictionary\n",
    "            if target in line:\n",
    "                \n",
    "                #if fn is not already a key in results then add empty\n",
    "                if fn not in results:\n",
    "                    results[fn]= []\n",
    "                \n",
    "                #file name is key and adding the index of the line \n",
    "                #   where tareget appeared\n",
    "                # value is list that stores indices\n",
    "                results[fn].append(line_index)\n",
    "                \n",
    "    return results\n",
    "            \n",
    "#reducer function to be used in map_reduce\n",
    "def reduce_grep(lines1, lines2):\n",
    "    #combining two dictionaries\n",
    "    lines1.update(lines2)\n",
    "    return lines1\n",
    "\n",
    "#geenralising wth path argumet so it can work with custom paths\n",
    "def mapreduce_grep(path, num_processes):\n",
    "    # each iterable in for loop are the file name sin the path directory\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep, reduce_grep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the occurence of \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = \"data\"\n",
    "data_occurrences = mapreduce_grep(\"wiki\", 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allowing for case insensitive matches\n",
    "We can allow case insensitive matches by converting both the target string and the file contents to lowercase before we match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_insensitive(file_names):\n",
    "    #dictionary that stores the results\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        #open each file and read each line in lower case\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        #enumerate helps us use index as one of iterables    \n",
    "        for line_index, line in enumerate(lines):\n",
    "            #target is available globally\n",
    "            if target.lower() in line:\n",
    "                #fn is key in the results\n",
    "                #values are just indices where target appears\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                #index is appended to a list\n",
    "                #this list contains indices where target appears\n",
    "                results[fn].append(line_index)\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_insensitive(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_insensitive, reduce_grep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"data\"\n",
    "new_data_occurrences = mapreduce_grep_insensitive(\"wiki\", 8)\n",
    "#print(new_data_occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if both implementations has different result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 new matches on file wiki/Table_Point_Formation.html\n",
      "Found 1 new matches on file wiki/Ingrid_GuimarC3A3es.html\n",
      "Found 2 new matches on file wiki/Jules_Verne_ATV.html\n",
      "Found 1 new matches on file wiki/Pictogram.html\n",
      "Found 2 new matches on file wiki/Claire_Danes.html\n",
      "Found 1 new matches on file wiki/PTPRS.html\n",
      "Found 1 new matches on file wiki/A_Beautiful_Valley.html\n",
      "Found 1 new matches on file wiki/Mudramothiram.html\n",
      "Found 2 new matches on file wiki/Gordon_Bau.html\n",
      "Found 1 new matches on file wiki/Embraer_Unidade_GaviC3A3o_Peixoto_Airport.html\n",
      "Found 3 new matches on file wiki/Code_page_1023.html\n",
      "Found 1 new matches on file wiki/Cryptographic_primitive.html\n",
      "Found 1 new matches on file wiki/Alex_Kurtzman.html\n",
      "Found 1 new matches on file wiki/Filip_Pyrochta.html\n",
      "Found 1 new matches on file wiki/Morgana_King.html\n",
      "Found 1 new matches on file wiki/Don_Parsons_(ice_hockey).html\n",
      "Found 1 new matches on file wiki/Bias.html\n",
      "Found 2 new matches on file wiki/Tomohiko_ItC58D_(director).html\n",
      "Found 1 new matches on file wiki/Imperial_Venus_(film).html\n",
      "Found 1 new matches on file wiki/Camp_Nelson_Confederate_Cemetery.html\n",
      "Found 1 new matches on file wiki/Benny_Lee.html\n",
      "Found 1 new matches on file wiki/Kul_Gul.html\n",
      "Found 1 new matches on file wiki/Medicago_murex.html\n",
      "Found 1 new matches on file wiki/Oldfield_Baby_Great_Lakes.html\n",
      "Found 1 new matches on file wiki/Wilson_Global_Explorer.html\n",
      "Found 1 new matches on file wiki/Craig_Chester.html\n",
      "Found 1 new matches on file wiki/Derek_Acorah.html\n",
      "Found 1 new matches on file wiki/Jack_Goes_Home.html\n",
      "Found 1 new matches on file wiki/Morning_Glory_(2010_film).html\n",
      "Found 1 new matches on file wiki/Tim_Spencer_(singer).html\n",
      "Found 1 new matches on file wiki/Lower_Blackburn_Grade_Bridge.html\n",
      "Found 1 new matches on file wiki/1953E2809354_FA_Cup_qualifying_rounds.html\n",
      "Found 1 new matches on file wiki/Sol_Eclipse.html\n",
      "Found 1 new matches on file wiki/Jonathan_A._Goldstein.html\n",
      "Found 1 new matches on file wiki/83_(number).html\n",
      "Found 1 new matches on file wiki/Devil_on_Horseback.html\n",
      "Found 1 new matches on file wiki/Harry_Hill_Bandholtz.html\n",
      "Found 2 new matches on file wiki/Shpolskii_matrix.html\n",
      "Found 6 new matches on file wiki/Dragnet_(franchise).html\n",
      "Found 1 new matches on file wiki/Qalat_Kat.html\n",
      "Found 3 new matches on file wiki/Maniitsoq_structure.html\n",
      "Found 1 new matches on file wiki/Ordinary_Virginia.html\n",
      "Found 1 new matches on file wiki/Dewoitine_D.21.html\n",
      "Found 1 new matches on file wiki/Furto_di_sera_bel_colpo_si_spera.html\n",
      "Found 1 new matches on file wiki/Rudy_The_Rudy_Giuliani_Story.html\n",
      "Found 1 new matches on file wiki/Exploratorium_(film).html\n",
      "Found 1 new matches on file wiki/Foulonia.html\n",
      "Found 1 new matches on file wiki/Amborella.html\n",
      "Found 1 new matches on file wiki/Rally_for_Democracy_and_Progress_(Benin).html\n",
      "Found 1 new matches on file wiki/Swathi_Chinukulu.html\n",
      "Found 2 new matches on file wiki/Precorrin6A_reductase.html\n",
      "Found 1 new matches on file wiki/The_Gentleman_Without_a_Residence_(1915_film).html\n",
      "Found 1 new matches on file wiki/Manhattan_Murder_Mystery.html\n",
      "Found 2 new matches on file wiki/Viva_Villa.html\n",
      "Found 1 new matches on file wiki/Companys_procC3A9s_a_Catalunya.html\n",
      "Found 1 new matches on file wiki/Avengers_Academy.html\n",
      "Found 1 new matches on file wiki/Antibiotic_use_in_livestock.html\n",
      "Found 1 new matches on file wiki/Syngenor.html\n",
      "Found 1 new matches on file wiki/Cobble_Hill_Brooklyn.html\n",
      "Found 1 new matches on file wiki/Typhoon_Hester_(1952).html\n",
      "Found 1 new matches on file wiki/WintersWimberley_House.html\n",
      "Found 1 new matches on file wiki/Kokan_Colony.html\n",
      "Found 1 new matches on file wiki/Wilhelm_Wagenfeld_House.html\n",
      "Found 2 new matches on file wiki/Taipa_HousesE28093Museum.html\n",
      "Found 1 new matches on file wiki/WLSR.html\n",
      "Found 1 new matches on file wiki/Lake_County_Examiner.html\n",
      "Found 1 new matches on file wiki/Copamyntis_infusella.html\n",
      "Found 1 new matches on file wiki/C11orf30.html\n",
      "Found 1 new matches on file wiki/Old_Mill_Creek_Illinois.html\n",
      "Found 1 new matches on file wiki/Bahmanabade_Olya.html\n",
      "Found 1 new matches on file wiki/Ek_Dil_Sau_Afsane.html\n",
      "Found 1 new matches on file wiki/Daniel_Cerone.html\n",
      "Found 1 new matches on file wiki/Shoreyjehye_Do.html\n",
      "Found 1 new matches on file wiki/Failing_Office_Building.html\n",
      "Found 1 new matches on file wiki/Pushkar.html\n",
      "Found 1 new matches on file wiki/List_of_Uzbek_films_of_2014.html\n",
      "Found 1 new matches on file wiki/KMTZ.html\n",
      "Found 1 new matches on file wiki/Golabkhvaran.html\n",
      "Found 1 new matches on file wiki/CurtissWright_Hangar_(Columbia_South_Carolina).html\n",
      "Found 1 new matches on file wiki/Blue_SWAT.html\n",
      "Found 1 new matches on file wiki/Danish_Maritime_Safety_Administration.html\n",
      "Found 1 new matches on file wiki/Don_Raye.html\n",
      "Found 1 new matches on file wiki/Lis_LC3B8wert.html\n",
      "Found 1 new matches on file wiki/Doumanaba.html\n",
      "Found 1 new matches on file wiki/Sahanpur.html\n",
      "Found 1 new matches on file wiki/Meleh_Kabude_Sofla.html\n",
      "Found 1 new matches on file wiki/Panchamrutham.html\n",
      "Found 1 new matches on file wiki/Bibiana_Beglau.html\n",
      "Found 1 new matches on file wiki/Kattukukke.html\n",
      "Found 1 new matches on file wiki/Acceptance_(Heroes).html\n",
      "Found 1 new matches on file wiki/Westchester_Los_Angeles.html\n",
      "Found 1 new matches on file wiki/Appa_(film).html\n",
      "Found 1 new matches on file wiki/HD_90156.html\n",
      "Found 2 new matches on file wiki/The_Audacity_to_Podcast.html\n",
      "Found 1 new matches on file wiki/Brownfield_(software_development).html\n",
      "Found 1 new matches on file wiki/Boardman_Township_Mahoning_County_Ohio.html\n",
      "Found 1 new matches on file wiki/King_Parker_House.html\n",
      "Found 2 new matches on file wiki/List_of_Spaghetti_Western_films.html\n",
      "Found 1 new matches on file wiki/The_Future_(film).html\n",
      "Found 1 new matches on file wiki/Weiser_River.html\n",
      "Found 1 new matches on file wiki/Jon_Mullich.html\n",
      "Found 1 new matches on file wiki/Saravan_Gilan.html\n",
      "Found 2 new matches on file wiki/Agaritine_gammaglutamyltransferase.html\n",
      "Found 1 new matches on file wiki/Nuno_Leal_Maia.html\n",
      "Found 1 new matches on file wiki/Battle_of_Wattignies.html\n",
      "Found 1 new matches on file wiki/Colchester_Village_Historic_District.html\n",
      "Found 1 new matches on file wiki/Hayateumi_Hidehito.html\n",
      "Found 7 new matches on file wiki/List_of_people_from_Bangor_Maine.html\n",
      "Found 1 new matches on file wiki/Mirisah.html\n",
      "Found 1 new matches on file wiki/Teiji_Ito.html\n",
      "Found 1 new matches on file wiki/L._Fry.html\n",
      "Found 1 new matches on file wiki/Tropical_sprue.html\n",
      "Found 1 new matches on file wiki/Roxbury_Presbyterian_Church.html\n",
      "Found 1 new matches on file wiki/Peter_Collingwood.html\n",
      "Found 4 new matches on file wiki/List_of_molecular_graphics_systems.html\n",
      "Found 1 new matches on file wiki/Functoid.html\n",
      "Found 1 new matches on file wiki/Vojin_C486etkoviC487.html\n",
      "Found 1 new matches on file wiki/Julien_Boisselier.html\n",
      "Found 1 new matches on file wiki/Jazz_in_Turkey.html\n",
      "Found 2 new matches on file wiki/Kim_Yonghwa.html\n",
      "Found 1 new matches on file wiki/Holly_Golightly_(comics).html\n",
      "Found 1 new matches on file wiki/SalemAuburn_Streets_Historic_District.html\n",
      "Found 2 new matches on file wiki/Kate_Harwood.html\n",
      "Found 1 new matches on file wiki/Gulliver_Mickey.html\n",
      "Found 1 new matches on file wiki/Urs_Burkart.html\n",
      "Found 1 new matches on file wiki/Smilax_laurifolia.html\n",
      "Found 1 new matches on file wiki/Taylor_Williamson.html\n",
      "Found 1 new matches on file wiki/Claudia_Neidig.html\n",
      "Found 1 new matches on file wiki/Dean_Kukan.html\n",
      "Found 1 new matches on file wiki/Demographics_of_American_Samoa.html\n",
      "Found 1 new matches on file wiki/C389cole_des_Mines_de_Douai.html\n",
      "Found 1 new matches on file wiki/Frost_Township_Michigan.html\n",
      "Found 1 new matches on file wiki/Shabbir_Kumar.html\n",
      "Found 1 new matches on file wiki/West_Park_Bridge.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for fn in new_data_occurrences:\n",
    "    #checking filename not in data_occurence\n",
    "    if fn not in data_occurrences:\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]), fn))\n",
    "    #checking if length of list is different in each implementation\n",
    "    #rem: list has indices where target string apprears\n",
    "    elif len(new_data_occurrences[fn]) > len(data_occurrences[fn]):\n",
    "        print(\"Found {} new matches on file {}\".format(len(new_data_occurrences[fn]) - len(data_occurrences[fn]), fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subfunction that stores the indices within a line where \"target\" substring appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 14, 30]\n"
     ]
    }
   ],
   "source": [
    "def find_match_indices(line, target):\n",
    "    results = []\n",
    "    # str.find(target, start index, end index)\n",
    "    #-1 is returned if there is no match\n",
    "    i = line.find(target, 0)\n",
    "    #loops till end of the string\n",
    "    while i != -1:\n",
    "        results.append(i)\n",
    "        #fiding if the target appears in the next index \n",
    "        i = line.find(target, i + 1)\n",
    "    return results\n",
    "\n",
    "# Test implementation\n",
    "s = \"Data science ,data mining,big data.\".lower()\n",
    "print(find_match_indices(s, \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding all match locations \n",
    "\n",
    "We can use the above function in combination with previously created function to store file name , the line withing the filename where \"target\" appears and the indices in the line where the \"target\" appears "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grep_match_indices(file_names):\n",
    "    #dictionary that stores the results\n",
    "    results = {}\n",
    "    for fn in file_names:\n",
    "        #open each file and read each line in lower case\n",
    "        with open(fn) as f:\n",
    "            lines = [line.lower() for line in f.readlines()]\n",
    "        #enumerate helps us use index as one of iterables    \n",
    "        for line_index, line in enumerate(lines):\n",
    "            #target is available globally\n",
    "            match_indices = find_match_indices(line,target.lower())\n",
    "            if target.lower() in line:\n",
    "                #fn is key in the results\n",
    "                #values are just indices where target appears\n",
    "                if fn not in results:\n",
    "                    results[fn] = []\n",
    "                #index is appended to a list\n",
    "                #this list contains indices where target appears\n",
    "                results[fn]+= [(line_index, match_index) for match_index in match_indices]\n",
    "    return results\n",
    "\n",
    "def mapreduce_grep_match_indices(path, num_processes):\n",
    "    file_names = [os.path.join(path, fn) for fn in os.listdir(path)]\n",
    "    return map_reduce(file_names, num_processes,  map_grep_match_indices, reduce_grep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding \"science\" in given list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"science\"\n",
    "occurrences = mapreduce_grep_match_indices(\"wiki\", 8)\n",
    "#import itertools\n",
    "#N=3\n",
    "#out = dict(itertools.islice(occurrences.items(), N)) \n",
    "#print(str(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying the results\n",
    "We will create a CSV file listing all occurrences, with File, line, index and context where the word\"science\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# How many character to show before and after the match\n",
    "context_delta = 30\n",
    "\n",
    "with open(\"results.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    rows = [[\"File\", \"Line\", \"Index\", \"Context\"]]\n",
    "    #iterating over key values in dictionary\n",
    "    for fn in occurrences: \n",
    "        with open(fn) as f:\n",
    "            lines = [line.strip() for line in f.readlines()]\n",
    "        for line, index in occurrences[fn]:\n",
    "            #index for start and end of string\n",
    "            start = max(index - context_delta, 0)\n",
    "            end   = index + len(target) + context_delta\n",
    "            rows.append([fn, line, index, lines[line][start:end]])\n",
    "        \n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "      <th>Index</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>840</td>\n",
       "      <td>embers of the USSR Academy of Sciences\",\"Full ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>6</td>\n",
       "      <td>890</td>\n",
       "      <td>ers of the Russian Academy of Sciences\",\"Demid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>90</td>\n",
       "      <td>href=\"/wiki/Soviet_Academy_of_Sciences\" class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>145</td>\n",
       "      <td>ect\" title=\"Soviet Academy of Sciences\"&gt;Soviet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>66</td>\n",
       "      <td>173</td>\n",
       "      <td>f Sciences\"&gt;Soviet Academy of Sciences&lt;/a&gt;; he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1440</td>\n",
       "      <td>rs_of_the_USSR_Academy_of_Sciences\" title=\"Cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1502</td>\n",
       "      <td>rs of the USSR Academy of Sciences\"&gt;Full Membe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1548</td>\n",
       "      <td>rs of the USSR Academy of Sciences&lt;/a&gt;&lt;/li&gt;&lt;li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1632</td>\n",
       "      <td>of_the_Russian_Academy_of_Sciences\" title=\"Cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>wiki/Valentin_Yanin.html</td>\n",
       "      <td>144</td>\n",
       "      <td>1697</td>\n",
       "      <td>of the Russian Academy of Sciences\"&gt;Full Membe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File  Line  Index  \\\n",
       "0  wiki/Valentin_Yanin.html     6    840   \n",
       "1  wiki/Valentin_Yanin.html     6    890   \n",
       "2  wiki/Valentin_Yanin.html    66     90   \n",
       "3  wiki/Valentin_Yanin.html    66    145   \n",
       "4  wiki/Valentin_Yanin.html    66    173   \n",
       "5  wiki/Valentin_Yanin.html   144   1440   \n",
       "6  wiki/Valentin_Yanin.html   144   1502   \n",
       "7  wiki/Valentin_Yanin.html   144   1548   \n",
       "8  wiki/Valentin_Yanin.html   144   1632   \n",
       "9  wiki/Valentin_Yanin.html   144   1697   \n",
       "\n",
       "                                             Context  \n",
       "0  embers of the USSR Academy of Sciences\",\"Full ...  \n",
       "1  ers of the Russian Academy of Sciences\",\"Demid...  \n",
       "2  href=\"/wiki/Soviet_Academy_of_Sciences\" class=...  \n",
       "3  ect\" title=\"Soviet Academy of Sciences\">Soviet...  \n",
       "4  f Sciences\">Soviet Academy of Sciences</a>; he...  \n",
       "5  rs_of_the_USSR_Academy_of_Sciences\" title=\"Cat...  \n",
       "6  rs of the USSR Academy of Sciences\">Full Membe...  \n",
       "7  rs of the USSR Academy of Sciences</a></li><li...  \n",
       "8  of_the_Russian_Academy_of_Sciences\" title=\"Cat...  \n",
       "9  of the Russian Academy of Sciences\">Full Membe...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_csv(\"results.csv\")\n",
    "df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
